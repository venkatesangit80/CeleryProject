import redis
import threading
from concurrent.futures import ThreadPoolExecutor

# Initialize Redis client
r = redis.StrictRedis(host='localhost', port=6379, db=0)

# Pattern for Redis keys (replace with your actual key pattern)
pattern = 'metrics_*'

# Function to fetch data using Redis pipeline and write it to a separate file
def fetch_and_write_to_file(keys, file_name):
    # Create a pipeline to fetch data in bulk
    pipeline = r.pipeline()
    for key in keys:
        pipeline.get(key)
    
    # Execute the pipeline
    values = pipeline.execute()

    # Write the fetched data to the file for this thread
    with open(file_name, 'w') as f:
        for value in values:
            if value:
                f.write(value.decode('utf-8') + '\n')  # Write to the file without a lock, since each thread has its own file

# Function to scan Redis keys and distribute the work to multiple threads
def process_keys_in_batches(base_file_name, batch_size=100):
    cursor = 0
    keys = []
    thread_count = 0  # To give unique file names to each thread

    # Create a thread pool executor for parallel processing
    with ThreadPoolExecutor(max_workers=4) as executor:
        while True:
            cursor, new_keys = r.scan(cursor=cursor, match=pattern, count=batch_size)
            keys.extend(new_keys)

            # When the number of keys reaches the batch size, process them in parallel
            if len(keys) >= batch_size:
                # Each thread writes to its own file
                thread_file_name = f"{base_file_name}_thread_{thread_count}.txt"
                executor.submit(fetch_and_write_to_file, keys[:batch_size], thread_file_name)

                # Clear the processed keys and keep the unprocessed ones
                keys = keys[batch_size:]
                thread_count += 1

            # If no more keys, break
            if cursor == 0:
                break

        # Process remaining keys that didn't fill the batch size
        if keys:
            thread_file_name = f"{base_file_name}_thread_{thread_count}.txt"
            executor.submit(fetch_and_write_to_file, keys, thread_file_name)

# Main function to start the processing
if __name__ == '__main__':
    base_file_name = 'vrops_metrics_data'
    
    # Start processing Redis keys and writing to separate files per thread
    process_keys_in_batches(base_file_name, batch_size=100)


import os

# List of all the thread-specific files
file_names = [f'vrops_metrics_data_thread_{i}.txt' for i in range(thread_count)]

# Merge the files into one
with open('merged_vrops_metrics_data.txt', 'w') as outfile:
    for fname in file_names:
        with open(fname) as infile:
            outfile.write(infile.read())
