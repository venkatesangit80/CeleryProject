import pandas as pd
import numpy as np

# Sample DataFrame
data = {
    'A': [np.nan, 2, np.nan, 4],
    'B': [1, np.nan, 3, np.nan],
    'C': [np.nan, np.nan, np.nan, 5]
}
df = pd.DataFrame(data)

# Select the first non-null value from A, B, or C
df['ColumnA'] = df[['A', 'B', 'C']].bfill(axis=1).iloc[:, 0]

print(df)



import pandas as pd

# Sample DataFrame
data = {
    'app': ['app1', 'app1', 'app1', 'app2', 'app2', 'app2', 'app3', 'app3', 'app3'],
    'env': ['dev', 'test', 'prod', 'dev', 'test', 'prod', 'dev', 'test', 'prod'],
    'host': [10, 15, 20, 5, 10, 15, 12, 18, 24],
    'TAS': [5, 7, 9, 2, 4, 6, 6, 8, 10],
    'network': [3, 4, 5, 1, 2, 3, 2, 3, 4],
    'DB': [8, 10, 12, 4, 5, 6, 7, 9, 11]
}

df = pd.DataFrame(data)

# Group by 'app' and sum values for all resources
all_env = df.groupby('app').sum(numeric_only=True).reset_index()
all_env['env'] = 'all'  # Add 'all' as the environment

# Concatenate the original DataFrame with the aggregated 'all' environment
result = pd.concat([df, all_env], ignore_index=True)

# Reorder the DataFrame
result = result[['app', 'env', 'host', 'TAS', 'network', 'DB']]

# Sort the DataFrame for better readability
result = result.sort_values(by=['app', 'env']).reset_index(drop=True)

print(result)

SELECT 
    SUBSTR(column_name, 1, INSTR(column_name, '-', -1) - 1) AS extracted_value
FROM 
    table_name
WHERE 
    INSTR(column_name, '-') > 0;





MERGE INTO target_table t
USING source_table s
ON (t.id = s.id)
WHEN MATCHED THEN
    UPDATE SET 
        t.value = s.value,        -- Update existing data
        t.is_active = 'Y',        -- Ensure record is active
        t.deleted_at = NULL       -- Clear soft delete if reactivated
WHEN NOT MATCHED THEN
    INSERT (id, value, is_active, deleted_at)
    VALUES (s.id, s.value, 'Y', NULL)
WHEN NOT MATCHED BY SOURCE THEN
    UPDATE SET 
        t.is_active = 'N',        -- Mark as soft deleted
        t.deleted_at = SYSDATE    -- Record the soft delete timestamp
    WHERE t.is_active = 'Y';      -- Only soft delete active records



DELETE FROM target_table
WHERE is_active = 'N'
  AND deleted_at <= SYSDATE - 5; -- Deletes records older than 5 days



import pandas as pd
import numpy as np

# Sample DataFrame
data = {
    'environment': ['Prod', 'DR', 'Test', 'Prod'],
    'host_count': [10, 0, 5, 0],
    'tas_count': [0, 0, 0, 5],
    'tkgi_count': [0, 0, 0, 0],
    'cloud_service_type': ['NO SERVICE', 'NO SERVICE', 'NO SERVICE', 'NO SERVICE']
}
df = pd.DataFrame(data)

# Define the conditions
conditions = [
    (df['environment'].isin(['Prod', 'DR'])) & (df['cloud_service_type'] == 'NO SERVICE') & (df['host_count'] > 0) & (df['tas_count'] == 0) & (df['tkgi_count'] == 0),
    (df['environment'].isin(['Prod', 'DR'])) & (df['cloud_service_type'] == 'NO SERVICE') & (df['host_count'] == 0) & ((df['tas_count'] > 0) | (df['tkgi_count'] > 0)),
    (df['environment'].isin(['Prod', 'DR'])) & (df['cloud_service_type'] == 'NO SERVICE') & (df['host_count'] > 0) & ((df['tas_count'] > 0) | (df['tkgi_count'] > 0))
]

# Define the corresponding outputs
outputs = ['VM/PM', 'PAAS', 'HYBRID']

# Apply the conditions and assign the new values
df['cloud_service_type'] = np.select(conditions, outputs, default=df['cloud_service_type'])

print(df)


MERGE INTO platform p
USING platform_staging ps
ON (p.unique_id = ps.unique_id)
WHEN MATCHED THEN
    UPDATE SET
        p.lob = ps.lob,
        p.appid = ps.appid,
        p.environment = ps.environment,
        p.platform_data = ps.platform_data,
        p.is_active = 'Y',
        p.deleted_at = NULL
WHEN NOT MATCHED THEN
    INSERT (unique_id, lob, appid, environment, platform_data, is_active, deleted_at)
    VALUES (ps.unique_id, ps.lob, ps.appid, ps.environment, ps.platform_data, 'Y', NULL)
WHEN NOT MATCHED BY SOURCE THEN
    UPDATE SET
        p.is_active = 'N',
        p.deleted_at = SYSTIMESTAMP;


BEGIN
    -- Merge operation for inserts and updates
    MERGE INTO platform p
    USING platform_staging ps
    ON (p.unique_id = ps.unique_id)
    WHEN MATCHED THEN
        UPDATE SET
            p.lob = ps.lob,
            p.appid = ps.appid,
            p.environment = ps.environment,
            p.platform_data = ps.platform_data,
            p.is_active = 'Y',
            p.deleted_at = NULL
    WHEN NOT MATCHED THEN
        INSERT (unique_id, lob, appid, environment, platform_data, is_active, deleted_at)
        VALUES (ps.unique_id, ps.lob, ps.appid, ps.environment, ps.platform_data, 'Y', NULL);

    -- Update operation for soft deletion
    UPDATE platform p
    SET p.is_active = 'N',
        p.deleted_at = SYSTIMESTAMP
    WHERE NOT EXISTS (
        SELECT 1
        FROM platform_staging ps
        WHERE ps.unique_id = p.unique_id
    );
END;



Here’s a well-structured outline for your PowerPoint presentation to showcase your achievements and next steps to the top management:

Slide 1: Title Slide

Title: Achievements and Ongoing Initiatives
Subtitle: Driving Innovation and Efficiency in IT Operations
Your Name
Date

Slide 2: Agenda
	1.	Workload Implementation Overview
	2.	Celery-Based Multi-Process Platform for VROPs Metrics
	3.	Anomaly Detection Proof of Concept
	4.	Distant Pattern Suppression Algorithm
	5.	Next Action Items: Custom Tier and Threshold Configuration

Slide 3: Workload Implementation

Objective: Identify whether servers (VM/PM) are active or passive using ML models.
	•	Accomplishments:
	•	Machine learning model implemented to classify servers as active or passive.
	•	Aggregated data at Data Center, Application, and LOB levels.
	•	Delivered end-to-end CSV report generation for external communication.
	•	Outcome:
	•	Improved visibility and reporting accuracy for workload classification.
	•	Enhanced decision-making for external stakeholders.

Slide 4: Celery-Based Multi-Process Platform

Objective: Faster execution of VROPs metrics loading to Prometheus.
	•	Accomplishments:
	•	Developed a multi-process Celery platform for metric extraction and transformation.
	•	Handled 4 GB of metric data efficiently (pending Prometheus fixes).
	•	Solution designed for scalability, awaiting infrastructure procurement.
	•	Outcome:
	•	Expected reduction in data processing time.
	•	Scalable and optimized metric ingestion pipeline.

Slide 5: Anomaly Detection Proof of Concept

Objective: Implement POC for anomaly detection in IT operations.
	•	Progress:
	•	Conducted the first meeting with the Elastic Search team.
	•	Scheduled the second meeting with Gireesh’s team to finalize POC requirements.
	•	Next Steps:
	•	Initiate POC development.
	•	Evaluate and refine the model for production-readiness.

Slide 6: Distant Pattern Suppression

Objective: Improve accuracy in utilization calculations with advanced ML algorithms.
	•	Accomplishments:
	•	Implemented a KMeans-based machine learning algorithm.
	•	Enhanced over- and under-utilization calculations by suppressing distant patterns.
	•	Outcome:
	•	Increased accuracy in workload utilization metrics.
	•	Enabled better capacity planning and optimization.

Slide 7: Next Action Items
	1.	Custom Tier and Threshold Configuration:
	•	Design a flexible configuration mechanism for users.
	•	Incorporate tier-based thresholds to improve monitoring precision.
	2.	Prometheus Fixes:
	•	Collaborate with Prometheus team to address data scraping challenges.
	3.	Infrastructure Procurement:
	•	Secure necessary infrastructure for full-scale deployment of Celery platform.

Slide 8: Challenges and Mitigation
	•	Challenges:
	•	Large data volumes for Prometheus scraping (4 GB).
	•	Infrastructure dependencies for Celery platform deployment.
	•	Mitigation:
	•	Engage with Prometheus support for optimizations.
	•	Work with procurement teams to expedite infrastructure acquisition.

Slide 9: Conclusion
	•	Demonstrated significant progress across diverse initiatives.
	•	Moving towards scalable, efficient, and AI-driven IT solutions.
	•	Ready to tackle next steps and deliver impactful results.

Slide 10: Q&A

Title: Questions and Discussion
Subtitle: Looking forward to your feedback and inputs.

Tips:
	•	Include visuals like charts, diagrams, and process flows for clarity.
	•	Highlight outcomes and value added to the organization for each initiative.
	•	Keep the language concise and focus on key achievements.
.
Let me know if you need help refining the content or designing the slides!



npm install react-redux@9.1.2

https://github.com/rt2zz/redux-persist/issues/1462


import json
import csv

OLD_FILE = 'old-deps.json'
NEW_FILE = 'new-deps.json'
OUTPUT_CSV = 'dependency-diff.csv'

def gather_dependencies(node, path=None):
    """
    Recursively gather dependencies from an npm ls --json object.
    Returns a dictionary mapping 'pathString' -> {name, version}.
    """
    if path is None:
        path = []

    dep_map = {}
    name = node.get('name', 'root')
    version = node.get('version', 'unknown')
    # If you'd like to include the root project itself, uncomment:
    # dep_map[' > '.join(path + [name])] = {'name': name, 'version': version}

    dependencies = node.get('dependencies', {})
    for dep_name, dep_obj in dependencies.items():
        dep_version = dep_obj.get('version', '')
        full_path = path + [name, dep_name]
        path_str = ' > '.join(full_path)
        dep_map[path_str] = {'name': dep_name, 'version': dep_version}
        # Recurse into the dependency's dependencies
        dep_map.update(gather_dependencies(dep_obj, path + [name, dep_name]))

    return dep_map

def main():
    with open(OLD_FILE, 'r') as f:
        old_data = json.load(f)
    with open(NEW_FILE, 'r') as f:
        new_data = json.load(f)

    old_map = gather_dependencies(old_data)
    new_map = gather_dependencies(new_data)

    differences = []

    # Check for removed or changed dependencies
    for dep_path, old_info in old_map.items():
        if dep_path not in new_map:
            # Removed
            differences.append({
                'type': 'removed',
                'path': dep_path,
                'oldVersion': old_info['version'],
                'newVersion': ''
            })
        else:
            # Check for version change
            new_version = new_map[dep_path]['version']
            if new_version != old_info['version']:
                differences.append({
                    'type': 'changed',
                    'path': dep_path,
                    'oldVersion': old_info['version'],
                    'newVersion': new_version
                })

    # Check for added dependencies
    for dep_path, new_info in new_map.items():
        if dep_path not in old_map:
            # Added
            differences.append({
                'type': 'added',
                'path': dep_path,
                'oldVersion': '',
                'newVersion': new_info['version']
            })

    # Write differences to CSV
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['type', 'path', 'oldVersion', 'newVersion'])
        for diff in differences:
            writer.writerow([
                diff['type'],
                diff['path'],
                diff['oldVersion'],
                diff['newVersion']
            ])

    print(f"Differences written to {OUTPUT_CSV}")

if __name__ == "__main__":
    main()



import json

INPUT_FILE = 'deps.json'

def main():
    with open(INPUT_FILE, 'r') as f:
        data = json.load(f)

    # The top-level dependencies are directly under data["dependencies"] if they exist
    top_level_deps = data.get("dependencies", {})

    # Construct a dictionary of dependency: version pairs
    # In npm ls output, each dependency object should have a "version"
    deps_dict = {}
    for dep_name, dep_info in top_level_deps.items():
        version = dep_info.get("version", "")
        if version:
            deps_dict[dep_name] = version
        else:
            # If no version found, fallback or skip
            # Sometimes a peer dependency or unresolved dependency might not have a version
            continue

    # Create a snippet that can be part of package.json
    package_section = {
        "dependencies": deps_dict
    }

    # Print in pretty JSON format
    print(json.dumps(package_section, indent=2))

if __name__ == "__main__":
    main()


import pandas as pd
from datetime import datetime, timedelta

# Generate a date range for 365 days starting from today
start_date = datetime.today()
date_range = [start_date + timedelta(days=i) for i in range(365)]

# Create a dataframe with 'datetime' and 'p95' columns
data = {
    "datetime": [date.strftime('%Y-%m-%d') for date in date_range],
    "p95": [10] * 365
}

df = pd.DataFrame(data)

# Display the dataframe to the user
import ace_tools as tools; tools.display_dataframe_to_user(name="Generated DataFrame with DateTime and p95 Values", dataframe=df)